{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib qt5\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "'''read datasets from file and prepare vector values'''\n",
    "def prepareInputData(path, typ=float, fsize=5):\n",
    "    lines = open(path).read().strip().split(\"\\n\")\n",
    "    # prepare float or input type values from space separated string \n",
    "    out = [uniform_filter1d([typ(a) for a in re.split('\\s+', lines[i].strip())], size=fsize) for i in range(len(lines))]\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "'''return a list of names of datasets and a list of prepared datasets'''\n",
    "def loadDataVectors(data_dir, file_patt, prepareFn):\n",
    "    datafiles = glob.glob(os.sep.join((data_dir, file_patt)))\n",
    "    datafiles.sort()\n",
    "    # had to use literal separator\n",
    "    names = [name[name.rindex('\\\\')+1:name.rindex('.')] for name in datafiles]\n",
    "    data = []\n",
    "    for datafile in datafiles:\n",
    "        data.append(prepareFn(datafile))\n",
    "    return names, data\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "'''Show crosstabulation \n",
    "    field1 - list of values\n",
    "    field2 - list of values correlating to field1\n",
    "    field1_map - if field1 is a list of ints, map is a list of labels with index= field1 - 1\n",
    "    field2_map - if field2 is a list of ints, map is a list of labels with index= field2 - 1'''\n",
    "def showCrossTab(field1, field_name1, field2, field_name2, field1_map=None, field2_map=None):\n",
    "    fig = plt.figure(figsize=(15,10),tight_layout=True, dpi=80)\n",
    "    if field1_map != None:\n",
    "        field1 = [field1_map[int(x) - 1] for x in field1]\n",
    "    if field2_map != None:\n",
    "        field2 = [y for y in map(lambda x: field2_map[int(x) - 1], field2)]\n",
    "    \n",
    "    xtab = pd.DataFrame({field_name1:field1, field_name2:field2})\n",
    "    sns.heatmap(pd.crosstab(xtab.iloc[:,0], xtab.iloc[:,1], margins=True), square=True,cmap=\"YlGnBu\", annot=True, fmt = \"d\", cbar=False)\n",
    "\n",
    "# phone data vectors - each row contains 128 values (2.56 secs at 50Hz)\n",
    "\n",
    "# feature vectors - 561 values per row\n",
    "# raw data is loaded in loadOrCreateDTWfeatures only if it hasn't been processed before\n",
    "test_feat_path = 'UCI HAR Dataset/test'\n",
    "train_feat_path = 'UCI HAR Dataset/train'\n",
    "test_featset_names, test_feat = loadDataVectors(test_feat_path, \"X*.txt\", prepareInputData)\n",
    "train_featset_names, train_feat = loadDataVectors(train_feat_path, \"X*.txt\", prepareInputData)\n",
    "ypath = 'UCI HAR Dataset/test/y_test.txt'\n",
    "y_test = open(ypath).read().strip().split(\"\\n\")\n",
    "ypath = 'UCI HAR Dataset/train/y_train.txt'\n",
    "y_train = open(ypath).read().strip().split(\"\\n\")\n",
    "\n",
    "fnamepath = 'UCI HAR Dataset/features.txt'\n",
    "feat_names = open(fnamepath).read().strip().split(\"\\n\")\n",
    "actpath = 'UCI HAR Dataset/activity_labels.txt'\n",
    "activities = open(actpath).read().strip().split(\"\\n\")\n",
    "# strip leading number in activity descriptions\n",
    "activities = [[a for a in re.split('[0-9]\\s+', x.strip()) if a!=''][0] for x in activities]\n",
    "\n",
    "\n",
    "subjpath = 'UCI HAR Dataset/train/subject_train.txt'\n",
    "subj_train = open(subjpath).read().strip().split(\"\\n\")\n",
    "subjpath = 'UCI HAR Dataset/test/subject_test.txt'\n",
    "subj_test = open(subjpath).read().strip().split(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# set up a dataframe with the subject and action for crosstabulation\n",
    "\n",
    "showCrossTab( y_train, \"action\", subj_train, \"subject\", field1_map=activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtw\n",
    "import random\n",
    "def createActionList(labelset, x, seed=12):\n",
    "    random.seed = seed\n",
    "    typeset = list(set(labelset)) * x\n",
    "    typeset.sort()\n",
    "    actions = [labelset.index(a, random.randint(labelset.index(a), len(labelset) - labelset[::-1].index(a) - 1)) for a in typeset]\n",
    "    return actions\n",
    "\n",
    "def createDTWmat(querydata, traindata, actionlist, n):\n",
    "    # data is a tuple of matrices for attribute creation - acc and gyro data\n",
    "    # for each sample compare with a set of samples of the same data set and store distance\n",
    "    lens = [len(d) for d in querydata]\n",
    "    if not lens.count(lens[0]) == len(lens):\n",
    "        raise ValueError(\"query datasets must be of same length\")\n",
    "    lens = [len(d) for d in traindata]\n",
    "    if not lens.count(lens[0]) == len(lens):\n",
    "        raise ValueError(\"training datasets must be of same length\")\n",
    "    \n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(querydata[0])):\n",
    "        row = []\n",
    "        # each dataset\n",
    "        for j in range(len(querydata)):\n",
    "            query = querydata[j][i]\n",
    "            row += (findDTWdists(query, traindata[j], actionlist, n))\n",
    "        distances.append(row)\n",
    "    return distances\n",
    "\n",
    "def findDTWdists(query, traindata, actionlist, n):\n",
    "    #query = data\n",
    "    # random set of templates?\n",
    "    #actionlist = createActionList(labelset, x)\n",
    "    row = []\n",
    "    dist = 0\n",
    "    for pos, action in enumerate(actionlist):\n",
    "        template = traindata[action]\n",
    "        alignment = dtw.dtw(query, template) #,keep_internals=True)\n",
    "        \n",
    "        # take the mean of distances for a single action type and add it to the output list\n",
    "        if pos !=0 and pos % n == 0:\n",
    "            row.append(dist/n)\n",
    "            dist = 0\n",
    "        dist += alignment.normalizedDistance\n",
    "    # add last mean distance\n",
    "    row.append(dist/n)    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def loadOrCreateDTWfeatures(test_rawdata_path=None, train_rawdata_path=None, y_train=None, num_samples_for_DTWtemplate=None):\n",
    "    # try to read already stored values. If there are no such files then run the feature extraction and store the files\n",
    "    # TO Do - check input params\n",
    "    try:\n",
    "        filehandler = open(\"actionlist.pkl\", 'rb') \n",
    "        actionlist = pickle.load(filehandler)\n",
    "        filehandler = open(\"train_distances.pkl\", 'rb') \n",
    "        train_distances = pickle.load(filehandler)\n",
    "        filehandler = open(\"test_distances.pkl\", 'rb') \n",
    "        test_distances = pickle.load(filehandler)\n",
    "\n",
    "\n",
    "    except FileExistsError:\n",
    "        _, testdatasets = loadDataVectors(test_rawdata_path, \"*.txt\", prepareInputData)\n",
    "        _, traindatasets = loadDataVectors(train_rawdata_path, \"*.txt\", prepareInputData)\n",
    "\n",
    "\n",
    "        actionlist = createActionList(y_train, num_samples_for_DTWtemplate)\n",
    "        #traindatasets = (body_acc_x1_train, body_acc_y1_train, body_acc_z1_train, \\\n",
    "                         #body_gyro_x1_train, body_gyro_y1_train, body_gyro_z1_train, \\\n",
    "                        #total_acc_x1_train, total_acc_y1_train, total_acc_z1_train)\n",
    "        #testdatasets = (body_acc_x1_test, body_acc_y1_test, body_acc_z1_test, \\\n",
    "                        #body_gyro_x1_test, body_gyro_y1_test, body_gyro_z1_test, \\\n",
    "                       #total_acc_x1_test, total_acc_y1_test, total_acc_z1_test)\n",
    "\n",
    "        train_distances = createDTWmat(traindatasets, traindatasets, actionlist, num_samples_for_DTWtemplate)\n",
    "        test_distances = createDTWmat(testdatasets, traindatasets, actionlist, num_samples_for_DTWtemplate)\n",
    "\n",
    "        filehandler = open(actionlist.pkl, 'xb') \n",
    "        actionlist = pickle.dump(filehandler)\n",
    "        filehandler = open(train_distances.pkl, 'xb') \n",
    "        train_distances = pickle.dump(filehandler)\n",
    "        filehandler = open(test_distances.pkl, 'xb') \n",
    "        test_distances = pickle.dump(filehandler)\n",
    "    return train_distances, test_distances, actionlist\n",
    "        \n",
    "# set a value for the number of sample DTW distance values to aggregate\n",
    "num_samples_for_DTWtemplate = 5\n",
    "test_rawdata_path = 'UCI HAR Dataset/test/Inertial Signals'\n",
    "train_rawdata_path = 'UCI HAR Dataset/train/Inertial Signals'\n",
    "train_distances, test_distances, actionlist = loadOrCreateDTWfeatures(test_rawdata_path, train_rawdata_path, y_train, num_samples_for_DTWtemplate)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run plots to show vector similarities across samples\n",
    "def multiPlotVecors(actionlist, activities, y_train, rawdata_path, rawdatafilename):\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(15,10),tight_layout=True, dpi=80)\n",
    "    #plt.subplot\n",
    "    \n",
    "    datasetnames, datasets = loadDataVectors(rawdata_path, rawdatafilename, prepareInputData)\n",
    "    i = 0\n",
    "    for pos, action in enumerate(actionlist[::5]):\n",
    "        #plt.subplot(3,6,pos + 4*int(pos/2) + 1)\n",
    "        plt.subplot(6,5,pos + 4*int(pos) + 1)\n",
    "        x = datasets[i][action]\n",
    "        #xs = uniform_filter1d(total_acc_z1_train[action], size=5)\n",
    "        #x = range(len(y))\n",
    "        activity = activities[int(y_train[action]) - 1]\n",
    "        plt.title(activity)\n",
    "        plt.plot(x, 'g-', linewidth=2, markersize=8)\n",
    "        #plt.plot(xs, 'g-', color='red', linewidth=2, markersize=8)\n",
    "    for pos, action in enumerate(actionlist[1::5]):\n",
    "        #plt.subplot(3,6,pos + 4*int(pos/2) + 3)\n",
    "        plt.subplot(6,5,pos + 4*int(pos) + 2)\n",
    "        x = datasets[i][action]\n",
    "        #xs = uniform_filter1d(total_acc_z1_train[action], size=5)\n",
    "        #x = range(len(y))\n",
    "        activity = activities[int(y_train[action]) - 1]\n",
    "        plt.title(activity)\n",
    "        plt.plot(x, 'g-', color='green',linewidth=2, markersize=8)\n",
    "        #plt.plot(xs, 'g-', color='red',linewidth=2, markersize=8)\n",
    "    for pos, action in enumerate(actionlist[2::5]):\n",
    "        #plt.subplot(3,6,pos + 4*int(pos/2) + 5)\n",
    "        plt.subplot(6,5,pos + 4*int(pos) + 3)\n",
    "        x = datasets[i][action]\n",
    "        #xs = uniform_filter1d(total_acc_z1_train[action], size=5)\n",
    "        #x = range(len(y))\n",
    "        activity = activities[int(y_train[action]) - 1]\n",
    "        plt.title(activity)\n",
    "        plt.plot(x, 'g-', color='green',linewidth=2, markersize=8)\n",
    "        #plt.plot(xs, 'g-', color='red',linewidth=2, markersize=8)\n",
    "    for pos, action in enumerate(actionlist[3::5]):\n",
    "        #plt.subplot(3,6,pos + 4*int(pos/2) + 7)\n",
    "        plt.subplot(6,5,pos + 4*int(pos) + 4)\n",
    "        x = datasets[i][action]\n",
    "        #xs = uniform_filter1d(total_acc_z1_train[action], size=5)\n",
    "        #x = range(len(y))\n",
    "        activity = activities[int(y_train[action]) - 1]\n",
    "        plt.title(activity)\n",
    "        plt.plot(x, 'g-', color='green',linewidth=2, markersize=8)\n",
    "        #plt.plot(xs, 'g-', color='red',linewidth=2, markersize=8)\n",
    "    for pos, action in enumerate(actionlist[4::5]):\n",
    "        #plt.subplot(3,6,pos + 4*int(pos/2) + 9)\n",
    "        plt.subplot(6,5,pos + 4*int(pos) + 5)\n",
    "        x = datasets[i][action]\n",
    "        activity = activities[int(y_train[action]) - 1]\n",
    "        #xs = uniform_filter1d(total_acc_z1_train[action], size=5)\n",
    "        #x = range(len(y))\n",
    "        plt.title(activity)\n",
    "        plt.plot(x, 'g-', color='green',linewidth=2, markersize=8)\n",
    "        #plt.plot(xs, 'g-', color='red',linewidth=2, markersize=8)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "train_rawdata_path = 'UCI HAR Dataset/train/Inertial Signals'\n",
    "rawdatafilename = \"body_acc_y_train.txt\"\n",
    "multiPlotVecors(actionlist, activities, y_train, train_rawdata_path, rawdatafilename)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run MDS plot to show activity clusters \n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "plt.close()\n",
    "similarities = euclidean_distances(test_distances)\n",
    "mds = manifold.MDS( n_components=2, dissimilarity=\"precomputed\", random_state=6)\n",
    "results = mds.fit(similarities)\n",
    "coords = results.embedding_\n",
    "color = [int(c) -1 for c in y_test]\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sc = ax.scatter(coords[:, 0], coords[:, 1], marker = '.', c= color, cmap=\"tab10\")\n",
    "handles, labels = sc.legend_elements(prop=\"colors\")[0], activities\n",
    "plt.legend(handles,labels, loc='lower left', fontsize=\"x-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, recall_score, precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "\n",
    "# FIT models and return predictions for test input\n",
    "\n",
    "def fitKnn(X_train, X_test, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    scaled_trainfeatures = scaler.transform(X_train)\n",
    "    scaler.fit(X_test)\n",
    "    scaled_testfeatures = scaler.transform(X_test)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(scaled_trainfeatures, y_train)\n",
    "    y_pred_knn = knn.predict(scaled_testfeatures)\n",
    "    return knn, y_pred_knn\n",
    "\n",
    "\n",
    "def fitDecisionTree(X_train, X_test, y_train):\n",
    "    model = DecisionTreeClassifier(max_depth=5, min_samples_leaf=100, min_samples_split=100, max_leaf_nodes=8, random_state=2)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_dt = model.predict(X_test)\n",
    "\n",
    "    return model, y_pred_dt\n",
    "\n",
    "def fitLogisticRegression(X_train, X_test, y_train):\n",
    "    l = LogisticRegression(multi_class='ovr', max_iter=400)\n",
    "    # scale the data for regression for feature importance values\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    l.fit(X_train_scale, y_train)\n",
    "    y_pred_lr = l.predict(scaler.transform(X_test))\n",
    "    return l, y_pred_lr\n",
    "    \n",
    "def fitRandomForest(X_train, X_test, y_train):\n",
    "    r = RandomForestClassifier(n_estimators=100,max_depth=5, max_features=17, random_state=2)\n",
    "    r.fit(X_train,y_train)\n",
    "    y_pred_rf = r.predict(X_test)\n",
    "    return r, y_pred_rf\n",
    "\n",
    "def fitSVM(X_train, X_test, y_train):\n",
    "    svc = sklearn.svm.LinearSVC() \n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred_svm = svc.predict(X_test)\n",
    "    return svc, y_pred_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "nnmodel = Sequential()\n",
    "#model.add(tf.keras.Input(shape=(16,)))\n",
    "nnmodel.add(Dense(64, input_dim=len(train_distances[0]) , activation='relu'))\n",
    "nnmodel.add(Dense(64, activation='relu'))\n",
    "nnmodel.add(Dense(6, activation='sigmoid'))\n",
    "nnmodel.compile(optimizer = Adam(lr = 0.0005),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(nnmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn, y_pred_knn = fitKnn(train_distances, test_distances, y_train)\n",
    "dtmodel,y_pred_dt = fitDecisionTree(train_distances, test_distances, y_train)\n",
    "lr, y_pred_lr = fitLogisticRegression(train_distances, test_distances, y_train)\n",
    "rf, y_pred_rf = fitRandomForest(train_distances, test_distances, y_train)\n",
    "svc, y_pred_svm = fitSVM(train_distances, test_distances, y_train)\n",
    "y_pred_nn = nnmodel.predict(np.array(test_distances))\n",
    "y_pred_nn = np.argmax(y_pred_nn ,axis = 1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "CMknn = confusion_matrix(y_test, y_pred_knn)\n",
    "CMdt = confusion_matrix(y_test, y_pred_dt)\n",
    "CMlr = confusion_matrix(y_test, y_pred_lr)\n",
    "CMrf = confusion_matrix(y_test, y_pred_rf)\n",
    "CMsvm = confusion_matrix(y_test,y_pred_svm)\n",
    "y_true = np.argmax(by_test,axis = 1)\n",
    "CMnn = confusion_matrix(y_true, y_pred_nn)\n",
    "\n",
    "#print(CMlr)\n",
    "rep = sklearn.metrics.classification_report(y_test, y_pred_lr, target_names=activities)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(25,10),tight_layout=True, dpi=80)\n",
    "fig.tight_layout(h_pad=5)\n",
    "ax = plt.subplot(2,3,1)\n",
    "ax.set_title(\"KNN \\n predicted\")\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred_knn, average=None)\n",
    "precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "sns.heatmap(CMknn.data,cmap='Greens',annot=CMknn, fmt='',cbar=False, xticklabels=precisionvals, yticklabels=activities)\n",
    "\n",
    "ax = plt.subplot(2,3,2)\n",
    "ax.set_title(\"Decision Trees \\n predicted\")\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred_dt, average=None)\n",
    "precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "sns.heatmap(CMdt.data,cmap='Greens',annot=CMdt, fmt='',cbar=False, xticklabels=precisionvals, yticklabels=activities)\n",
    "\n",
    "ax = plt.subplot(2,3,3)\n",
    "ax.set_title = \"Logistic Regression \\n predicted\"\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred_lr, average=None)\n",
    "precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "sns.heatmap(CMlr.data,cmap='Greens',annot=CMlr, fmt='',cbar=False, xticklabels=precisionvals, yticklabels=activities)\n",
    "\n",
    "ax = plt.subplot(2,3,4)\n",
    "ax.set_title = \"Random Forests \\n predicted\"\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred_rf, average=None)\n",
    "precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "sns.heatmap(CMrf.data,cmap='Greens',annot=CMrf, fmt='',cbar=False, xticklabels=precisionvals, yticklabels=activities)\n",
    "\n",
    "ax = plt.subplot(2,3,5)\n",
    "ax.set_title = \"Support Vector Machines \\n predicted\"\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred_svm, average=None)\n",
    "precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "sns.heatmap(CMsvm.data,cmap='Greens',annot=CMsvm, fmt='',cbar=False, xticklabels=precisionvals, yticklabels=activities)\n",
    "\n",
    "ax = plt.subplot(2,3,6)\n",
    "ax.set_title = \"Neural Network \\n predicted\"\n",
    "precision = sklearn.metrics.precision_score(y_true, y_pred_nn, average=None)\n",
    "precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "sns.heatmap(CMnn.data,cmap='Greens',annot=CMnn, fmt='',cbar=False, xticklabels=precisionvals, yticklabels=activities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(25,10),tight_layout=True, dpi=80)\n",
    "\n",
    "def plotconfusion(ax, CM, y_test, y_pred, activities):\n",
    "    recall = sklearn.metrics.recall_score(y_test, y_pred, average=None)\n",
    "    precision = sklearn.metrics.precision_score(y_test, y_pred, average=None)\n",
    "\n",
    "    ax.annotate(\"RECALL\",(6.0,0.0))\n",
    "    ax.annotate(\"PREC.\",(0.0,6.0))\n",
    "    for x in range(len(recall)):  \n",
    "        ax.annotate(\"{:3.2f}\".format(recall[x]),(6.0,0.5 + x))\n",
    "    precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "    sns.heatmap(CM.data,cmap='Greens',annot=CM, fmt='',cbar=False,xticklabels=precisionvals, yticklabels=activities)\n",
    "    \n",
    "ax = plt.subplot(3,2,1)\n",
    "plotconfusion(ax, CMknn, y_test, y_pred_knn, activities)\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plotconfusion(ax, CMdt, y_test, y_pred_dt, activities)\n",
    "plt.subplot(3,2,3)\n",
    "plotconfusion(ax, CMlr, y_test, y_pred_lr, activities)\n",
    "plt.subplot(3,2,4)\n",
    "plotconfusion(ax, CMrf, y_test, y_pred_rf, activities)\n",
    "plt.subplot(3,2,5)\n",
    "plotconfusion(ax, CMsvm, y_test, y_pred_svm, activities)\n",
    "plt.subplot(3,2,6)\n",
    "plotconfusion(ax, CMnn, y_true, y_pred_nn, activities)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
