{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib qt5\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "'''read datasets from file and prepare vector values'''\n",
    "def prepareInputData(path, typ=float, fsize=5):\n",
    "    lines = open(path).read().strip().split(\"\\n\")\n",
    "    # prepare float or input type values from space separated string \n",
    "    out = [uniform_filter1d([typ(a) for a in re.split('\\s+', lines[i].strip())], size=fsize) for i in range(len(lines))]\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "'''return a list of names of datasets and a list of prepared datasets'''\n",
    "def loadDataVectors(data_dir, file_patt, prepareFn):\n",
    "    datafiles = glob.glob(os.sep.join((data_dir, file_patt)))\n",
    "    datafiles.sort()\n",
    "    # had to use literal separator\n",
    "    names = [name[name.rindex('\\\\')+1:name.rindex('.')] for name in datafiles]\n",
    "    data = []\n",
    "    for datafile in datafiles:\n",
    "        data.append(prepareFn(datafile))\n",
    "    return names, data\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "'''Show crosstabulation \n",
    "    field1 - list of values\n",
    "    field2 - list of values correlating to field1\n",
    "    field1_map - if field1 is a list of ints, map is a list of labels with index= field1 - 1\n",
    "    field2_map - if field2 is a list of ints, map is a list of labels with index= field2 - 1'''\n",
    "def showCrossTab(field1, field_name1, field2, field_name2, field1_map=None, field2_map=None):\n",
    "    fig = plt.figure(figsize=(15,10),tight_layout=True, dpi=80)\n",
    "    if field1_map != None:\n",
    "        field1 = [field1_map[int(x) - 1] for x in field1]\n",
    "    if field2_map != None:\n",
    "        field2 = [y for y in map(lambda x: field2_map[int(x) - 1], field2)]\n",
    "    \n",
    "    xtab = pd.DataFrame({field_name1:field1, field_name2:field2})\n",
    "    sns.heatmap(pd.crosstab(xtab.iloc[:,0], xtab.iloc[:,1], margins=True), square=True,cmap=\"YlGnBu\", annot=True, fmt = \"d\", cbar=False)\n",
    "\n",
    "# phone data vectors - each row contains 128 values (2.56 secs at 50Hz)\n",
    "\n",
    "# feature vectors - 561 values per row\n",
    "# raw data is loaded in loadOrCreateDTWfeatures only if it hasn't been processed before\n",
    "test_feat_path = 'UCI HAR Dataset/test'\n",
    "train_feat_path = 'UCI HAR Dataset/train'\n",
    "test_featset_names, test_feat = loadDataVectors(test_feat_path, \"X*.txt\", prepareInputData)\n",
    "train_featset_names, train_feat = loadDataVectors(train_feat_path, \"X*.txt\", prepareInputData)\n",
    "ypath = 'UCI HAR Dataset/test/y_test.txt'\n",
    "y_test = open(ypath).read().strip().split(\"\\n\")\n",
    "ypath = 'UCI HAR Dataset/train/y_train.txt'\n",
    "y_train = open(ypath).read().strip().split(\"\\n\")\n",
    "\n",
    "fnamepath = 'UCI HAR Dataset/features.txt'\n",
    "feat_names = open(fnamepath).read().strip().split(\"\\n\")\n",
    "actpath = 'UCI HAR Dataset/activity_labels.txt'\n",
    "activities = open(actpath).read().strip().split(\"\\n\")\n",
    "# strip leading number in activity descriptions\n",
    "activities = [[a for a in re.split('[0-9]\\s+', x.strip()) if a!=''][0] for x in activities]\n",
    "\n",
    "\n",
    "subjpath = 'UCI HAR Dataset/train/subject_train.txt'\n",
    "subj_train = open(subjpath).read().strip().split(\"\\n\")\n",
    "subjpath = 'UCI HAR Dataset/test/subject_test.txt'\n",
    "subj_test = open(subjpath).read().strip().split(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# set up a dataframe with the subject and action for crosstabulation\n",
    "\n",
    "showCrossTab( y_train, \"action\", subj_train, \"subject\", field1_map=activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtw\n",
    "import random\n",
    "def createActionList(labelset, x, seed=12):\n",
    "    random.seed = seed\n",
    "    typeset = list(set(labelset)) * x\n",
    "    typeset.sort()\n",
    "    actions = [labelset.index(a, random.randint(labelset.index(a), len(labelset) - labelset[::-1].index(a) - 1)) for a in typeset]\n",
    "    return actions\n",
    "\n",
    "def createDTWmat(querydata, traindata, actionlist, n):\n",
    "    # data is a tuple of matrices for attribute creation - acc and gyro data\n",
    "    # for each sample compare with a set of samples of the same data set and store distance\n",
    "    lens = [len(d) for d in querydata]\n",
    "    if not lens.count(lens[0]) == len(lens):\n",
    "        raise ValueError(\"query datasets must be of same length\")\n",
    "    lens = [len(d) for d in traindata]\n",
    "    if not lens.count(lens[0]) == len(lens):\n",
    "        raise ValueError(\"training datasets must be of same length\")\n",
    "    \n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(querydata[0])):\n",
    "        row = []\n",
    "        # each dataset\n",
    "        for j in range(len(querydata)):\n",
    "            query = querydata[j][i]\n",
    "            row += (findDTWdists(query, traindata[j], actionlist, n))\n",
    "        distances.append(row)\n",
    "    return distances\n",
    "\n",
    "def findDTWdists(query, traindata, actionlist, n):\n",
    "    #query = data\n",
    "    # random set of templates?\n",
    "    #actionlist = createActionList(labelset, x)\n",
    "    row = []\n",
    "    dist = 0\n",
    "    for pos, action in enumerate(actionlist):\n",
    "        template = traindata[action]\n",
    "        alignment = dtw.dtw(query, template) #,keep_internals=True)\n",
    "        \n",
    "        # take the mean of distances for a single action type and add it to the output list\n",
    "        if pos !=0 and pos % n == 0:\n",
    "            row.append(dist/n)\n",
    "            dist = 0\n",
    "        dist += alignment.normalizedDistance\n",
    "    # add last mean distance\n",
    "    row.append(dist/n)    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def loadOrCreateDTWfeatures(test_rawdata_path=None, train_rawdata_path=None, prepDataFn=None, y_train=None, num_samples_for_DTWtemplate=None):\n",
    "    # try to read already stored values. If there are no such files then run the feature extraction and store the files\n",
    "    # TO Do - check input params\n",
    "    try:\n",
    "        filehandler = open(\"actionlist.pkl\", 'rb') \n",
    "        actionlist = pickle.load(filehandler)\n",
    "        filehandler = open(\"train_distances.pkl\", 'rb') \n",
    "        train_distances = pickle.load(filehandler)\n",
    "        filehandler = open(\"test_distances.pkl\", 'rb') \n",
    "        test_distances = pickle.load(filehandler)\n",
    "\n",
    "\n",
    "    except FileExistsError:\n",
    "        _, testdatasets = loadDataVectors(test_rawdata_path, \"*.txt\", prepDataFn)\n",
    "        _, traindatasets = loadDataVectors(train_rawdata_path, \"*.txt\", prepDataFn)\n",
    "\n",
    "\n",
    "        actionlist = createActionList(y_train, num_samples_for_DTWtemplate)\n",
    "        #traindatasets = (body_acc_x1_train, body_acc_y1_train, body_acc_z1_train, \\\n",
    "                         #body_gyro_x1_train, body_gyro_y1_train, body_gyro_z1_train, \\\n",
    "                        #total_acc_x1_train, total_acc_y1_train, total_acc_z1_train)\n",
    "        #testdatasets = (body_acc_x1_test, body_acc_y1_test, body_acc_z1_test, \\\n",
    "                        #body_gyro_x1_test, body_gyro_y1_test, body_gyro_z1_test, \\\n",
    "                       #total_acc_x1_test, total_acc_y1_test, total_acc_z1_test)\n",
    "\n",
    "        train_distances = createDTWmat(traindatasets, traindatasets, actionlist, num_samples_for_DTWtemplate)\n",
    "        test_distances = createDTWmat(testdatasets, traindatasets, actionlist, num_samples_for_DTWtemplate)\n",
    "\n",
    "        filehandler = open(actionlist.pkl, 'xb') \n",
    "        actionlist = pickle.dump(filehandler)\n",
    "        filehandler = open(train_distances.pkl, 'xb') \n",
    "        train_distances = pickle.dump(filehandler)\n",
    "        filehandler = open(test_distances.pkl, 'xb') \n",
    "        test_distances = pickle.dump(filehandler)\n",
    "    return train_distances, test_distances, actionlist\n",
    "        \n",
    "# set a value for the number of sample DTW distance values to aggregate\n",
    "num_samples_for_DTWtemplate = 5\n",
    "test_rawdata_path = 'UCI HAR Dataset/test/Inertial Signals'\n",
    "train_rawdata_path = 'UCI HAR Dataset/train/Inertial Signals'\n",
    "train_distances, test_distances, actionlist = loadOrCreateDTWfeatures(test_rawdata_path, train_rawdata_path, prepareInputData, y_train, num_samples_for_DTWtemplate)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run plots to show vector similarities across samples\n",
    "'''show multiplot of actions from the actionlist list of rows for a chosen file\n",
    "     rawdatafilename should exist in rawdata_path directory'''\n",
    "def multiPlotVecors(actionlist, activities, y_train, rawdata_path, rawdatafilename, prepDataFn, num_samples=5):\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(15,10),tight_layout=True, dpi=80)\n",
    "    #plt.subplot\n",
    "    # TO do chexk inputs\n",
    "    datasetnames, datasets = loadDataVectors(rawdata_path, rawdatafilename, prepDataFn)\n",
    "    i = datasetnames.index(rawdatafilename[:rawdatafilename.rindex('.')])\n",
    "    for j in range(num_samples):\n",
    "        for pos, action in enumerate(actionlist[j::5]):\n",
    "            #plt.subplot(3,6,pos + 4*int(pos/2) + 1)\n",
    "            plt.subplot(6,5,pos + 4*int(pos) + j + 1)\n",
    "            x = datasets[i][action]\n",
    "            # plot a smoothed curve as well\n",
    "            #xs = uniform_filter1d(total_acc_z1_train[action], size=5)         \n",
    "            activity = activities[int(y_train[action]) - 1]\n",
    "            plt.title(activity)\n",
    "            plt.plot(x, 'g-', linewidth=2, markersize=8)\n",
    "            #plt.plot(xs, 'g-', color='red', linewidth=2, markersize=8)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "train_rawdata_path = 'UCI HAR Dataset/train/Inertial Signals'\n",
    "rawdatafilename = \"body_acc_y_train.txt\"\n",
    "multiPlotVecors(actionlist, activities, y_train, train_rawdata_path, rawdatafilename, prepareInputData, num_samples_for_DTWtemplate)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run MDS plot to show activity clusters \n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "'''scatterplot distance matrix with MDS showing different classes in colours\n",
    "    if classnameset is given for the legend, indices must be the class value -1'''\n",
    "def showMDSplot(dist_matrix, classes, classnameset={}):\n",
    "    plt.close()\n",
    "    similarities = euclidean_distances(dist_matrix)\n",
    "    mds = manifold.MDS( n_components=2, dissimilarity=\"precomputed\", random_state=6)\n",
    "    results = mds.fit(similarities)\n",
    "    coords = results.embedding_\n",
    "    color = [int(c) -1 for c in classes]\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    sc = ax.scatter(coords[:, 0], coords[:, 1], marker = '.', c= color, cmap=\"tab10\")\n",
    "    handles = sc.legend_elements(prop=\"colors\")[0]\n",
    "    plt.legend(handles, classnameset, loc='lower left', fontsize=\"x-large\")\n",
    "\n",
    "showMDSplot(test_distances, y_test, activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, recall_score, precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "\n",
    "# FIT models and return predictions for test input\n",
    "\n",
    "def fitKnn(X_train, X_test, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    scaled_trainfeatures = scaler.transform(X_train)\n",
    "    scaler.fit(X_test)\n",
    "    scaled_testfeatures = scaler.transform(X_test)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(scaled_trainfeatures, y_train)\n",
    "    y_pred_knn = knn.predict(scaled_testfeatures)\n",
    "    return knn, y_pred_knn\n",
    "\n",
    "\n",
    "def fitDecisionTree(X_train, X_test, y_train):\n",
    "    model = DecisionTreeClassifier(max_depth=5, min_samples_leaf=100, min_samples_split=100, max_leaf_nodes=8, random_state=2)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_dt = model.predict(X_test)\n",
    "\n",
    "    return model, y_pred_dt\n",
    "\n",
    "def fitLogisticRegression(X_train, X_test, y_train):\n",
    "    l = LogisticRegression(multi_class='ovr', max_iter=400)\n",
    "    # scale the data for regression for feature importance values\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    l.fit(X_train_scale, y_train)\n",
    "    y_pred_lr = l.predict(scaler.transform(X_test))\n",
    "    return l, y_pred_lr\n",
    "    \n",
    "def fitRandomForest(X_train, X_test, y_train):\n",
    "    r = RandomForestClassifier(n_estimators=100,max_depth=5, max_features=17, random_state=2)\n",
    "    r.fit(X_train,y_train)\n",
    "    y_pred_rf = r.predict(X_test)\n",
    "    return r, y_pred_rf\n",
    "\n",
    "def fitSVM(X_train, X_test, y_train):\n",
    "    svc = sklearn.svm.LinearSVC() \n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred_svm = svc.predict(X_test)\n",
    "    return svc, y_pred_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "    \n",
    "\n",
    "def constructNNmodel(input_len):\n",
    "    nnmodel = Sequential()\n",
    "    nnmodel.add(Dense(64, input_dim=input_len , activation='relu'))\n",
    "    nnmodel.add(Dense(64, activation='relu'))\n",
    "    nnmodel.add(Dense(6, activation='sigmoid'))\n",
    "    nnmodel.compile(optimizer = Adam(lr = 0.0005),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(nnmodel.summary())\n",
    "    return nnmodel\n",
    "    \n",
    "def fitNN(x_train, x_test, y_train):\n",
    "    # one hot classes\n",
    "    by_train = to_categorical(pd.DataFrame([int(y) -1 for y in y_train]) , num_classes=6, dtype=int)\n",
    "    nnmodel = constructNNmodel(len(x_test[0]))\n",
    "    history = nnmodel.fit(np.array(x_train), by_train , epochs=50 , batch_size = 256 ) \n",
    "    y_pred_nn = nnmodel.predict(x_test)\n",
    "    # return one hot to class string\n",
    "    y_pred_nn = [str(y+1) for y in np.argmax(y_pred_nn,axis = 1)]\n",
    "    return nnmodel, y_pred_nn\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 64)                3520      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 8,070\n",
      "Trainable params: 8,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.7143 - accuracy: 0.3126\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.4626 - accuracy: 0.4150\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.2412 - accuracy: 0.4855\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.0184 - accuracy: 0.6231\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.8488 - accuracy: 0.7376\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.7443 - accuracy: 0.7867\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.8017\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.6165 - accuracy: 0.8031\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5794 - accuracy: 0.8024\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.5515 - accuracy: 0.7994\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.8034\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4936 - accuracy: 0.8146\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4715 - accuracy: 0.8217\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.8246\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8296\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8313\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4176 - accuracy: 0.8371\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.4060 - accuracy: 0.8396\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8473\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.8467\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3747 - accuracy: 0.8514\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8510\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8501\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8581\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8484\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8611\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3496 - accuracy: 0.8525\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3443 - accuracy: 0.8579\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8528\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3449 - accuracy: 0.8540\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8620\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8575\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8596\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.8656\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8675\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8709\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8637\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8579\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8665\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8653\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8655\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.8741\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8726\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3027 - accuracy: 0.8746\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8730\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2965 - accuracy: 0.8710\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8700\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8804\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2931 - accuracy: 0.8740\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2941 - accuracy: 0.8812\n"
     ]
    }
   ],
   "source": [
    "knn, y_pred_knn = fitKnn(train_distances, test_distances, y_train)\n",
    "dtmodel,y_pred_dt = fitDecisionTree(train_distances, test_distances, y_train)\n",
    "lr, y_pred_lr = fitLogisticRegression(train_distances, test_distances, y_train)\n",
    "rf, y_pred_rf = fitRandomForest(train_distances, test_distances, y_train)\n",
    "svc, y_pred_svm = fitSVM(train_distances, test_distances, y_train)\n",
    "nnmodel, y_pred_nn = fitNN(train_distances, test_distances, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '4', '4', '4', '4', '4', '4', '4', '4', '4']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_nn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(25,10),tight_layout=True, dpi=80)\n",
    "\n",
    "def plotconfusion(ax, CM, y_test, y_pred, activities):\n",
    "    recall = sklearn.metrics.recall_score(y_test, y_pred, average=None)\n",
    "    precision = sklearn.metrics.precision_score(y_test, y_pred, average=None)\n",
    "\n",
    "    ax.annotate(\"RECALL\",(6.0,0.0))\n",
    "    ax.annotate(\"PREC.\",(0.0,6.0))\n",
    "    for x in range(len(recall)):  \n",
    "        ax.annotate(\"{:3.2f}\".format(recall[x]),(6.0,0.5 + x))\n",
    "    precisionvals = [\"{:3.2f}\".format(precision[x]) for x in range(len(precision))]\n",
    "    sns.heatmap(CM.data,cmap='Greens',annot=CM, fmt='',cbar=False,xticklabels=precisionvals, yticklabels=activities)\n",
    "\n",
    "\n",
    "def tabulateConfusion(y_test, y_pred_dict, activities):\n",
    "    CMknn = confusion_matrix(y_test, y_pred_dict[\"y_pred_knn\"])\n",
    "    CMdt = confusion_matrix(y_test, y_pred_dict[\"y_pred_dt\"])\n",
    "    CMlr = confusion_matrix(y_test, y_pred_dict[\"y_pred_lr\"])\n",
    "    CMrf = confusion_matrix(y_test, y_pred_dict[\"y_pred_rf\"])\n",
    "    CMsvm = confusion_matrix(y_test, y_pred_dict[\"y_pred_svm\"])\n",
    "    CMnn = confusion_matrix(y_test, y_pred_dict[\"y_pred_nn\"])\n",
    "\n",
    "        \n",
    "    ax = plt.subplot(3,2,1)\n",
    "    ax.set_title(\"kNN \\n predicted\")\n",
    "    plotconfusion(ax, CMknn, y_test, y_pred_knn, activities)\n",
    "\n",
    "    ax = plt.subplot(3,2,2)\n",
    "    ax.set_title(\"Decision Trees \\n predicted\")\n",
    "    plotconfusion(ax, CMdt, y_test, y_pred_dt, activities)\n",
    "    ax = plt.subplot(3,2,3)\n",
    "    ax.set_title(\"Logistic Regression \\n predicted\")\n",
    "    plotconfusion(ax, CMlr, y_test, y_pred_lr, activities)\n",
    "    ax = plt.subplot(3,2,4)\n",
    "    ax.set_title(\"Random Forests \\n predicted\")\n",
    "    plotconfusion(ax, CMrf, y_test, y_pred_rf, activities)\n",
    "    ax = plt.subplot(3,2,5)\n",
    "    ax.set_title(\"Support Vector Machines \\n predicted\")\n",
    "    plotconfusion(ax, CMsvm, y_test, y_pred_svm, activities)\n",
    "    ax = plt.subplot(3,2,6)\n",
    "    ax.set_title(\"Neural Network \\n predicted\")\n",
    "    plotconfusion(ax, CMnn, y_test, y_pred_nn, activities)\n",
    "    plt.show()\n",
    "\n",
    "y_pred_dict = {\"y_pred_knn\":y_pred_knn, \"y_pred_dt\":y_pred_dt, \"y_pred_lr\":y_pred_lr, \\\n",
    "               \"y_pred_rf\":y_pred_rf, \"y_pred_svm\":y_pred_svm, \"y_pred_nn\":y_pred_nn}\n",
    "tabulateConfusion(y_test, y_pred_dict, activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           WALKING       0.63      0.77      0.69       496\n",
      "  WALKING_UPSTAIRS       0.62      0.60      0.61       471\n",
      "WALKING_DOWNSTAIRS       0.79      0.62      0.70       420\n",
      "           SITTING       0.87      0.71      0.78       491\n",
      "          STANDING       0.78      0.90      0.84       532\n",
      "            LAYING       1.00      1.00      1.00       537\n",
      "\n",
      "          accuracy                           0.78      2947\n",
      "         macro avg       0.78      0.77      0.77      2947\n",
      "      weighted avg       0.79      0.78      0.78      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#print(CMlr)\n",
    "rep = sklearn.metrics.classification_report(y_test, y_pred_nn, target_names=activities)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
